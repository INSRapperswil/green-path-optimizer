[agent]
  interval = "10s"
  round_interval = true
  metric_buffer_limit = 10000
  flush_buffer_when_full = true
  collection_jitter = "0s"
  flush_interval = "10s"
  flush_jitter = "0s"
  debug = false
  quiet = false
  hostname = "telegraf"

# Netflow v5, Netflow v9 and IPFIX collector
[[inputs.netflow]]
  ## Address to listen for netflow,ipfix or sflow packets.
  service_address = "udp://:4739"

  ## Set the size of the operating system's receive buffer.
  ##   example: read_buffer_size = "64KiB"
  ## Uses the system's default if not set.
  read_buffer_size = "64KiB"

  ## Protocol version to use for decoding.
  ##   "ipfix"      -- IPFIX / Netflow v10 protocol (also works for Netflow v9)
  protocol = "ipfix"
  
[inputs.netflow.tags]
  export_type = "bucket"
  
[[processors.strings]]
  [[processors.strings.trim_left]]
    field = "ip_header_packet_section"
    cutset = "0x"

[[processors.converter]]
   [processors.converter.fields]
    unsigned = ["type_5053", "type_5054", "type_5055", "type_5056"]

# Conversion of specified IPFIX packet fields into tags
[[processors.converter]]
   [processors.converter.fields]
    tag = ["flow_label", "type_5050", "type_5052", "type_5053", "type_5054", "type_5055", "type_5056", "src", "dst", "src_port", "dst_port", "type_5060"]
    integer = ["type_5051"]

# Determine the export type of the metric and set the corresponding tag
[[processors.regex]]
  namepass = ["netflow"]

  [[processors.regex.tags]]
    key = "type_5050"
    pattern = '.*'
    result_key = "export_type"
    replacement = "aggregated_data_export"
    append = false

  [[processors.regex.tags]]
    key = "type_5060"
    pattern = '.*'
    result_key = "export_type"
    replacement = "raw_data_export"
    append = false

# Parsing raw_data_export field ip_header_packet_section
[[processors.starlark]]
    source = '''
def apply(metric):
    if metric.tags["export_type"] == "raw_data_export":
        data = metric.fields.pop("ip_header_packet_section")

        metric.tags["ip_version"] = str(int(data[0:1], 16))
        metric.fields["traffic_class"] = int(data[1:3], 16)
        metric.tags["flow_label"] = "0x" + str(data[3:9])
        metric.tags["source_ipv6"] = reformat_ipv6_address(str(data[16:48]))
        metric.tags["destination_ipv6"] = reformat_ipv6_address(str(data[48:80]))
        metric.tags["node_01"] = str(int(data[110:116], 16))
        metric.tags["node_02"] = str(int(data[118:124], 16))
        metric.tags["node_03"] = str(int(data[126:132], 16))
        metric.tags["node_04"] = str(int(data[134:140], 16))
        metric.fields["hop_limit_node_01"] = int(data[108:110], 16)
        metric.fields["hop_limit_node_02"] = int(data[116:118], 16)
        metric.fields["hop_limit_node_03"] = int(data[124:126], 16)
        metric.fields["hop_limit_node_04"] = int(data[132:134], 16)
        metric.tags["namespace_id"] = str(int(data[148:152], 16))
        metric.tags["flags"] = str(int(data[152:153], 16))
        metric.fields["ioam_data_param"] = int(data[156:162], 16)
        metric.tags["aggregator"] = str(int(data[162:164], 16))
        metric.fields["aggregate"] = int(data[164:172], 16)
        metric.tags["auxil_data_node_id"] = str(int(data[172:178], 16))
        metric.fields["hop_count"] = int(data[178:180], 16)

    return metric

def reformat_ipv6_address(string):
    chunks = [string[i:i+4] for i in range(0, len(string), 4)]
    formatted_string = ':'.join(chunks)
    return formatted_string
'''

# InfluxDB Bucket for the aggregated data export
[[outputs.influxdb_v2]]
  urls = ["http://192.168.100.10:8086"]
  organization = "OST"
  token = "DkX-6JeCjrHwlmbb6Ts2zxnHwFFNery4_YkufMAr4GSiESF187x-C9UkCciWNhfd8JqWro_9fo6-Sz7f3NuYpw=="
  bucket = "aggregated_data_export"
  [outputs.influxdb_v2.tagpass]
    export_type = ["aggregated_data_export"]

# InfluxDB Bucket for the raw data export
[[outputs.influxdb_v2]]
  urls = ["http://192.168.100.10:8086"]
  organization = "OST"
  token = "DkX-6JeCjrHwlmbb6Ts2zxnHwFFNery4_YkufMAr4GSiESF187x-C9UkCciWNhfd8JqWro_9fo6-Sz7f3NuYpw=="
  bucket = "raw_data_export"
  [outputs.influxdb_v2.tagpass]
    export_type = ["raw_data_export"]

[[outputs.file]]
    files = ["stdout"]
    rotation_interval = "1h"
    rotation_max_size = "20MB"
    rotation_max_archives = 3
    data_format = "json"